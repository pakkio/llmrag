# LLM RAG System Environment Configuration
# Copy this file to .env and fill in your actual values

# ===== REQUIRED =====
# OpenRouter API key for LLM analysis and explanations
# Get your key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# ===== LLM CONFIGURATION =====
# Language model for semantic analysis and content explanations
# Recommended models:
# - google/gemma-3n-e4b-it:free (NEW! innovative architecture, no hallucinations, FREE)
# - anthropic/claude-3-haiku:beta (fast, cost-effective)
# - google/gemini-2.0-flash-thinking-exp-01-21 (experimental, very fast, most cost-effective)
# - google/gemini-flash-2.0 (very fast, excellent for analysis)
# - anthropic/claude-3-sonnet:beta (balanced performance)
# - anthropic/claude-3-opus:beta (highest quality, slower)
# - meta-llama/llama-3.1-8b-instruct:free (free option)
# - mistralai/mistral-7b-instruct:free (free alternative)
SEMANTIC_MODEL=google/gemma-3n-e4b-it:free
# SEMANTIC_MODEL=anthropic/claude-3-haiku:beta  # Previous default, fast and cost-effective

# ===== OPENROUTER API CONFIGURATION =====
# Application URL for OpenRouter API headers
OPENROUTER_APP_URL=http://localhost

# Application title for OpenRouter API tracking
OPENROUTER_APP_TITLE=LLM_RAG_System

# ===== DEBUGGING =====
# Enable debug logging (true/false)
# When enabled, shows detailed API calls, file loading, and server status
PAK_DEBUG=false

# ===== EMBEDDING SERVER CONFIGURATION =====
# Note: These are handled by the llama.cpp server and scripts
# The embedding server runs on http://127.0.0.1:8080 by default
# Model: qwen3-embedding-0.6b-q8_0.gguf (located in llama.cpp/models/)
# 
# To change embedding models, edit start_embedding_server.sh
# and update the MODEL_PATH variable